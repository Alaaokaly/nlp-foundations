{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaaokaly/nlp-foundations/blob/main/RNNTextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4ySAPTd6TAEc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i-asCDA4UZKv"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Weather in Asia: A Diverse Tapestry of Climates Asia, the largest continent on Earth, is renowned for its immense diversity, not only in culture and geography but also in climate. From the icy tundras of Siberia to the tropical rainforests of Southeast Asia, the weather across this vast expanse varies dramatically. Understanding the climatic conditions in different regions of Asia is essential for agriculture, tourism, and daily life, influencing everything from crop cycles to travel plans.\n",
        "In northern Asia, particularly in Siberia, winters are harsh and long. Temperatures can plummet to as low as -40°C in some areas, making it one of the coldest places on Earth. The vast taiga forest is covered in snow for much of the year, creating a winter wonderland that attracts adventurous tourists. However, summers are brief and can be surprisingly warm, with temperatures reaching 30°C.\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDQgbizfRz-_",
        "outputId": "1a1b16b7-f331-4ad6-fc1f-d9624f5aa390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 889 characters, 33 unique.\n"
          ]
        }
      ],
      "source": [
        "data = corpus.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gKMAF4nYRE49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d821f99a-b9a6-489a-f660-23826abbd6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(33, 37, num_layers=5, batch_first=True)\n",
            "  (fc): Linear(in_features=37, out_features=33, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 120\n",
        "input_size = len(chars)   # Number of unique characters\n",
        "output_size = len(chars)  # Number of unique characters\n",
        "hidden_n = 37             # Hidden layer size\n",
        "sequence_length = 5       # Length of input sequences and note that the smaller the sequence the better the performance\n",
        "num_layers = 5            # Number of RNN layers\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h_0):\n",
        "        out, h_t = self.rnn(x, h_0)  # out: (batch_size, sequence_length, hidden_size)\n",
        "        out = out[:, -1, :]          # Use only the last output: (batch_size, hidden_size)\n",
        "        out = self.fc(out)           # Pass to fully connected layer\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "model = RNN(input_size, hidden_n, output_size, num_layers)\n",
        "\n",
        "# Move model to GPU if available (optional)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lnRDu3ClqbEv"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PyWu2yyOrCRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57114b9d-0ad7-44a2-f4a5-a57ebc10000d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  0\n",
            "Epoch 1, Loss: 3.0162\n",
            "Epoch :  1\n",
            "Epoch :  2\n",
            "Epoch :  3\n",
            "Epoch :  4\n",
            "Epoch :  6\n",
            "Epoch :  7\n",
            "Epoch :  8\n",
            "Epoch :  9\n",
            "Epoch :  10\n",
            "Epoch 11, Loss: 2.1141\n",
            "Epoch :  11\n",
            "Epoch :  12\n",
            "Epoch :  13\n",
            "Epoch :  14\n",
            "Epoch :  15\n",
            "Epoch :  16\n",
            "Epoch :  17\n",
            "Epoch :  18\n",
            "Epoch :  19\n",
            "Epoch :  20\n",
            "Epoch 21, Loss: 1.5414\n",
            "Epoch :  21\n",
            "Epoch :  22\n",
            "Epoch :  23\n",
            "Epoch :  24\n",
            "Epoch :  25\n",
            "Epoch :  26\n",
            "Epoch :  27\n",
            "Epoch :  28\n",
            "Epoch :  29\n",
            "Epoch :  30\n",
            "Epoch 31, Loss: 1.1605\n",
            "Epoch :  31\n",
            "Epoch :  32\n",
            "Epoch :  33\n",
            "Epoch :  34\n",
            "Epoch :  35\n",
            "Epoch :  36\n",
            "Epoch :  37\n",
            "Epoch :  38\n",
            "Epoch :  39\n",
            "Epoch :  40\n",
            "Epoch 41, Loss: 0.8175\n",
            "Epoch :  41\n",
            "Epoch :  42\n",
            "Epoch :  43\n",
            "Epoch :  44\n",
            "Epoch :  45\n",
            "Epoch :  46\n",
            "Epoch :  47\n",
            "Epoch :  48\n",
            "Epoch :  49\n",
            "Epoch :  50\n",
            "Epoch 51, Loss: 0.6469\n",
            "Epoch :  51\n",
            "Epoch :  52\n",
            "Epoch :  53\n",
            "Epoch :  54\n",
            "Epoch :  55\n",
            "Epoch :  56\n",
            "Epoch :  57\n",
            "Epoch :  58\n",
            "Epoch :  59\n",
            "Epoch :  60\n",
            "Epoch 61, Loss: 0.5618\n",
            "Epoch :  61\n",
            "Epoch :  62\n",
            "Epoch :  63\n",
            "Epoch :  64\n",
            "Epoch :  65\n",
            "Epoch :  66\n",
            "Epoch :  67\n",
            "Epoch :  68\n",
            "Epoch :  69\n",
            "Epoch :  70\n",
            "Epoch 71, Loss: 0.4840\n",
            "Epoch :  71\n",
            "Epoch :  72\n",
            "Epoch :  73\n",
            "Epoch :  74\n",
            "Epoch :  75\n",
            "Epoch :  76\n",
            "Epoch :  77\n",
            "Epoch :  78\n",
            "Epoch :  79\n",
            "Epoch :  80\n",
            "Epoch 81, Loss: 0.4868\n",
            "Epoch :  81\n",
            "Epoch :  82\n",
            "Epoch :  83\n",
            "Epoch :  84\n",
            "Epoch :  85\n",
            "Epoch :  86\n",
            "Epoch :  87\n",
            "Epoch :  88\n",
            "Epoch :  89\n",
            "Epoch :  90\n",
            "Epoch 91, Loss: 0.4682\n",
            "Epoch :  91\n",
            "Epoch :  92\n",
            "Epoch :  93\n",
            "Epoch :  94\n",
            "Epoch :  95\n",
            "Epoch :  96\n",
            "Epoch :  97\n",
            "Epoch :  98\n",
            "Epoch :  99\n",
            "Epoch :  100\n",
            "Epoch 101, Loss: 0.4776\n",
            "Epoch :  101\n",
            "Epoch :  102\n",
            "Epoch :  103\n",
            "Epoch :  104\n",
            "Epoch :  105\n",
            "Epoch :  106\n",
            "Epoch :  107\n",
            "Epoch :  108\n",
            "Epoch :  109\n",
            "Epoch :  110\n",
            "Epoch 111, Loss: 0.3547\n",
            "Epoch :  111\n",
            "Epoch :  112\n",
            "Epoch :  113\n",
            "Epoch :  114\n",
            "Epoch :  115\n",
            "Epoch :  116\n",
            "Epoch :  117\n",
            "Epoch :  118\n",
            "Epoch :  119\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    print('Epoch : ', epoch)\n",
        "    epoch_loss = 0\n",
        "    for i in range(len(corpus) - sequence_length):\n",
        "\n",
        "        # Prepare input and target sequences\n",
        "        x_seq = [char_to_idx[ch] for ch in data[i:i + sequence_length]]\n",
        "        y_seq = char_to_idx[data[i+sequence_length]]\n",
        "\n",
        "        # Convert to tensor and one-hot encode\n",
        "        x_tensor = torch.zeros(1, sequence_length, vocab_size)\n",
        "        for j, idx in enumerate(x_seq):\n",
        "            x_tensor[0, j, idx] = 1  # One-hot encoding\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h_0 = torch.zeros(num_layers, 1, hidden_n)   # Shape (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        y_pred = model(x_tensor, h_0)  # Get the prediction\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(y_pred, torch.tensor([y_seq]))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    if epoch%10 ==0:\n",
        "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / (len(corpus) - sequence_length):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w2XYcU-UsmBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4105c9ea-4219-4d83-dd6c-981aba47a0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'weath' -> Predicted next character: 'e'\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    test_input = \"weath\"\n",
        "    x_seq = [char_to_idx[ch] for ch in test_input]\n",
        "    x_tensor = torch.zeros(1, sequence_length, vocab_size)\n",
        "    for j, idx in enumerate(x_seq):\n",
        "        x_tensor[0, j, idx] = 1  # One-hot encoding\n",
        "\n",
        "    h_0 = torch.zeros(num_layers, 1, hidden_n)\n",
        "    predicted_output = model(x_tensor, h_0)\n",
        "\n",
        "    # Use temperature sampling\n",
        "    temperature = 0.8\n",
        "    probabilities = torch.softmax(predicted_output / temperature, dim=-1)\n",
        "    predicted_char_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
        "    predicted_char = idx_to_char[predicted_char_idx]\n",
        "\n",
        "    print(f\"Input: '{test_input}' -> Predicted next character: '{predicted_char}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGusklbu05qV"
      },
      "outputs": [],
      "source": [
        "# evaluate using preplexity"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPQnTcyT4VF6lt8fUaLT4Xl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}