{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 320111,
          "sourceType": "datasetVersion",
          "datasetId": 134715
        },
        {
          "sourceId": 9631341,
          "sourceType": "datasetVersion",
          "datasetId": 5879880
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook15adc08dcc",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaaokaly/nlp-foundations/blob/main/sentimentalAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "i-FPGTGlP9xj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "lakshmi25npathi_imdb_dataset_of_50k_movie_reviews_path = kagglehub.dataset_download('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')\n",
        "alaaokaly_proc_text_path = kagglehub.dataset_download('alaaokaly/proc-text')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "MzEMKZamP9xk"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-15T08:18:52.29746Z",
          "iopub.execute_input": "2024-10-15T08:18:52.298203Z",
          "iopub.status.idle": "2024-10-15T08:18:53.395264Z",
          "shell.execute_reply.started": "2024-10-15T08:18:52.298162Z",
          "shell.execute_reply": "2024-10-15T08:18:53.394236Z"
        },
        "trusted": true,
        "id": "yPlHtNVKP9xk",
        "outputId": "e820a7e9-d41f-49a3-faf3-4cb62f5a4635"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  model libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import torch.optim as optim\n",
        "\n",
        "## word procesing\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from collections import  Counter\n",
        "# misc\n",
        "from multiprocessing import Pool\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T11:51:22.41683Z",
          "iopub.execute_input": "2024-10-15T11:51:22.417289Z",
          "iopub.status.idle": "2024-10-15T11:51:22.702971Z",
          "shell.execute_reply.started": "2024-10-15T11:51:22.417247Z",
          "shell.execute_reply": "2024-10-15T11:51:22.701922Z"
        },
        "trusted": true,
        "id": "G_goPu0NP9xl",
        "outputId": "a205d452-9eb3-45c7-eb45-cf0c88b3d280"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 221,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T10:18:08.15891Z",
          "iopub.execute_input": "2024-10-15T10:18:08.159398Z",
          "iopub.status.idle": "2024-10-15T10:21:10.005783Z",
          "shell.execute_reply.started": "2024-10-15T10:18:08.159351Z",
          "shell.execute_reply": "2024-10-15T10:21:10.003826Z"
        },
        "trusted": true,
        "id": "ScJiw74nP9xl",
        "outputId": "3784dd80-c3c3-4604-efaf-9ec867d7a441"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2024-10-15 10:18:09--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2024-10-15 10:18:09--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2024-10-15 10:18:10--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: 'glove.6B.zip'\n\nglove.6B.zip        100%[===================>] 822.24M  5.15MB/s    in 2m 59s  \n\n2024-10-15 10:21:09 (4.59 MB/s) - 'glove.6B.zip' saved [862182613/862182613]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip\n",
        "!ls /kaggle/working"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T10:23:33.874728Z",
          "iopub.execute_input": "2024-10-15T10:23:33.875813Z",
          "iopub.status.idle": "2024-10-15T10:24:00.51544Z",
          "shell.execute_reply.started": "2024-10-15T10:23:33.875759Z",
          "shell.execute_reply": "2024-10-15T10:24:00.513977Z"
        },
        "trusted": true,
        "id": "kUmGPbFSP9xl",
        "outputId": "63e32f5d-7c13-4fed-ed99-198b3eb80299"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Archive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \nfeatures.csv\t   glove.6B.200d.txt  glove.6B.50d.txt\tprocessed_text.csv\nglove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:  # Use 'utf-8' or try 'ISO-8859-1' if necessary\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove_file = '/kaggle/working/glove.6B.100d.txt'  # Ensure this path is correct\n",
        "glove_embeddings = load_glove_embeddings(glove_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T10:33:47.458387Z",
          "iopub.execute_input": "2024-10-15T10:33:47.459483Z",
          "iopub.status.idle": "2024-10-15T10:33:56.913723Z",
          "shell.execute_reply.started": "2024-10-15T10:33:47.459435Z",
          "shell.execute_reply": "2024-10-15T10:33:56.912547Z"
        },
        "trusted": true,
        "id": "FAmgI-BsP9xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenization\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Removing stop words and lemmatization\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return str(words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T08:24:30.007505Z",
          "iopub.execute_input": "2024-10-15T08:24:30.007971Z",
          "iopub.status.idle": "2024-10-15T08:24:30.015828Z",
          "shell.execute_reply.started": "2024-10-15T08:24:30.007928Z",
          "shell.execute_reply": "2024-10-15T08:24:30.014688Z"
        },
        "trusted": true,
        "id": "AGDvAbcTP9xm",
        "outputId": "3be892d7-0cbd-4776-e5de-8aa3f9cfc302"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "data = pd.DataFrame(data)\n",
        "data.columns = ['text', 'label']\n",
        "data.label = data.label.map({'positive' : 1,\n",
        "                            'negative':0})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:21:47.556781Z",
          "iopub.execute_input": "2024-10-15T12:21:47.557791Z",
          "iopub.status.idle": "2024-10-15T12:21:48.284409Z",
          "shell.execute_reply.started": "2024-10-15T12:21:47.557746Z",
          "shell.execute_reply": "2024-10-15T12:21:48.283306Z"
        },
        "trusted": true,
        "id": "3s9eCyrNP9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = pd.read_csv('/kaggle/working/processed_text.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:23:18.612858Z",
          "iopub.execute_input": "2024-10-15T12:23:18.613311Z",
          "iopub.status.idle": "2024-10-15T12:23:19.144776Z",
          "shell.execute_reply.started": "2024-10-15T12:23:18.613268Z",
          "shell.execute_reply": "2024-10-15T12:23:19.143834Z"
        },
        "trusted": true,
        "id": "2I9cOK3oP9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess using list comprehension\n",
        "def worker(chunk):\n",
        "    return chunk['text'].apply(preprocess_text)\n",
        "\n",
        "# Main function to process the DataFrame\n",
        "def process_data_in_parallel(data, num_workers=4):\n",
        "    # Split the DataFrame into chunks\n",
        "    chunks = np.array_split(data, num_workers)\n",
        "\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(worker, chunks)\n",
        "\n",
        "    # Combine results back into a single DataFrame\n",
        "    return pd.concat(results)\n",
        "\n",
        "data['text'] = process_data_in_parallel(data, num_workers=25)\n",
        "data['text'] = list(data['text'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "RvxwfQGvP9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_pad(line, max_len):\n",
        "    # Truncate if longer than max_len\n",
        "    line = line[:max_len]\n",
        "    # Pad if shorter than max_len\n",
        "    return line + [38116] * (max_len - len(line))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:19:57.227221Z",
          "iopub.execute_input": "2024-10-15T12:19:57.228371Z",
          "iopub.status.idle": "2024-10-15T12:19:57.234204Z",
          "shell.execute_reply.started": "2024-10-15T12:19:57.228325Z",
          "shell.execute_reply": "2024-10-15T12:19:57.233024Z"
        },
        "trusted": true,
        "id": "rU-eTP4LP9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 500\n",
        "\n",
        "\n",
        "\n",
        "all_words = [word for line in data['text'] for word in line]\n",
        "word_counts = Counter(all_words)\n",
        "# Filter words that appear at least 5 times\n",
        "vocabulary = {word: i + 1 for i, (word, count) in enumerate(word_counts.items()) if count >= 5}\n",
        "vocabulary = {word: idx for idx, (word, idx) in enumerate(vocabulary.items()) if word in glove_embeddings}\n",
        "#vocabulary['<pad>'] = 0 # Padding token\n",
        "\n",
        "\n",
        "\n",
        "# Use word_tokenize on each line before looking up in vocabulary\n",
        "features = torch.tensor([\n",
        "    truncate_pad(\n",
        "        [vocabulary[word] for word in line if word in vocabulary],\n",
        "        num_steps\n",
        "    ) for line in data['text']  # Iterate over each line in the DataFrame\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:19:57.236403Z",
          "iopub.execute_input": "2024-10-15T12:19:57.236848Z",
          "iopub.status.idle": "2024-10-15T12:20:17.815173Z",
          "shell.execute_reply.started": "2024-10-15T12:19:57.236803Z",
          "shell.execute_reply": "2024-10-15T12:20:17.813953Z"
        },
        "trusted": true,
        "id": "MdQzwTmuP9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape, len(vocabulary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:20:17.818223Z",
          "iopub.execute_input": "2024-10-15T12:20:17.818625Z",
          "iopub.status.idle": "2024-10-15T12:20:17.825847Z",
          "shell.execute_reply.started": "2024-10-15T12:20:17.818571Z",
          "shell.execute_reply": "2024-10-15T12:20:17.82477Z"
        },
        "trusted": true,
        "id": "mVteL0gTP9xm",
        "outputId": "01f70d8c-8af3-4d42-e57c-a49a72243074"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 245,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([50000, 500]), 30)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings only for words present in glove_embeddings\n",
        "embeds = torch.tensor(\n",
        "    np.array([glove_embeddings[word] for word in vocabulary if word in glove_embeddings]),\n",
        "    dtype=torch.float\n",
        ")\n",
        "\n",
        "print(\"Embeddings shape:\", embeds.shape)\n",
        "\n",
        "# Optionally, check how many words were excluded\n",
        "missing_words = [word for word in vocabulary if word not in glove_embeddings]\n",
        "print(\"Number of missing words:\", embeds.shape[0] ,missing_words)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:20:17.827166Z",
          "iopub.execute_input": "2024-10-15T12:20:17.8275Z",
          "iopub.status.idle": "2024-10-15T12:20:17.838711Z",
          "shell.execute_reply.started": "2024-10-15T12:20:17.827465Z",
          "shell.execute_reply": "2024-10-15T12:20:17.837591Z"
        },
        "trusted": true,
        "id": "M-UANe7HP9xm",
        "outputId": "d1f86c56-90ef-41b1-d7b1-86115c007785"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Embeddings shape: torch.Size([30, 100])\nNumber of missing words: 30 []\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "\n",
        "features = torch.tensor(features.tolist(), dtype=torch.int)\n",
        "labels = torch.tensor(data['label'].tolist(), dtype=torch.long)  # Use long for integer labels\n",
        "\n",
        "# Create a full dataset\n",
        "full_dataset = TensorDataset(features, labels)\n",
        "\n",
        "# Create a DataLoader for the full dataset\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create a subset for the first 2500 samples\n",
        "subset_size = 2500\n",
        "if len(full_dataset) < subset_size:\n",
        "    subset_size = len(full_dataset)  # Adjust if the dataset has fewer than 2500 samples\n",
        "\n",
        "# Create a subset dataset and DataLoader\n",
        "subset_indices = list(range(subset_size))  # Get indices for the first 2500 samples\n",
        "subset_dataset = Subset(full_dataset, subset_indices)  # Properly create a subset dataset\n",
        "subset_dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:51:27.973917Z",
          "iopub.execute_input": "2024-10-15T12:51:27.974737Z",
          "iopub.status.idle": "2024-10-15T12:51:31.947845Z",
          "shell.execute_reply.started": "2024-10-15T12:51:27.974681Z",
          "shell.execute_reply": "2024-10-15T12:51:31.946694Z"
        },
        "trusted": true,
        "id": "7VAvlRyHP9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "print(\"Full dataset size:\", len(full_dataset))\n",
        "print(\"Subset dataset size:\", len(subset_dataset))\n",
        "print(\"Features shape:\", features.shape)\n",
        "print(\"Labels shape:\", labels.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:49:35.184813Z",
          "iopub.execute_input": "2024-10-15T12:49:35.185775Z",
          "iopub.status.idle": "2024-10-15T12:49:35.192045Z",
          "shell.execute_reply.started": "2024-10-15T12:49:35.185727Z",
          "shell.execute_reply": "2024-10-15T12:49:35.190918Z"
        },
        "trusted": true,
        "id": "j0ywAgE5P9xn",
        "outputId": "ed726da2-b28a-4732-f075-eae5645f4f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Features shape: torch.Size([50000, 500])\nLabels shape: torch.Size([50000])\nFull dataset size: 50000\nSubset dataset size: 2500\nFeatures shape: torch.Size([50000, 500])\nLabels shape: torch.Size([50000])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['label'].unique())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:49:06.874913Z",
          "iopub.execute_input": "2024-10-15T12:49:06.875367Z",
          "iopub.status.idle": "2024-10-15T12:49:06.882404Z",
          "shell.execute_reply.started": "2024-10-15T12:49:06.875325Z",
          "shell.execute_reply": "2024-10-15T12:49:06.881404Z"
        },
        "trusted": true,
        "id": "PCmJO_55P9xn",
        "outputId": "f1489000-9a27-4ec5-9944-70d3b6fb75d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[1 0]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic start with lstm\n",
        "# open for improvemnt\n",
        "\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, hidden_dim, embedding_dim, vocab_size, num_layers):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional = True )\n",
        "        self.decoder = nn.Linear (hidden_dim*2*num_layers, 2)\n",
        "\n",
        "    def forward (self, inputs) :\n",
        "        outputs = self.embedding (inputs.T)\n",
        "        self.encoder.flatten_parameters()\n",
        "        outputs, _ = self.encoder(outputs)\n",
        "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:23:38.790981Z",
          "iopub.execute_input": "2024-10-15T12:23:38.79202Z",
          "iopub.status.idle": "2024-10-15T12:23:38.802302Z",
          "shell.execute_reply.started": "2024-10-15T12:23:38.791964Z",
          "shell.execute_reply": "2024-10-15T12:23:38.801061Z"
        },
        "trusted": true,
        "id": "EQ3oa4kAP9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100\n",
        "num_layers = 2\n",
        "num_hidden = 100\n",
        "\n",
        "model = BiRNN(num_hidden,embedding_size,len(vocabulary),num_layers)\n",
        "model.apply(initialize_Weights)\n",
        "model.embedding.weight.data.copy_(embeds)\n",
        "model.embedding.weight.requires_grad = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:23:38.803892Z",
          "iopub.execute_input": "2024-10-15T12:23:38.804364Z",
          "iopub.status.idle": "2024-10-15T12:23:38.829346Z",
          "shell.execute_reply.started": "2024-10-15T12:23:38.804314Z",
          "shell.execute_reply": "2024-10-15T12:23:38.828281Z"
        },
        "trusted": true,
        "id": "thElBIigP9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_Weights (module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "    elif type(module) == nn.LSTM:\n",
        "        for param in module._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(module._parameters[param])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:23:38.832368Z",
          "iopub.execute_input": "2024-10-15T12:23:38.832911Z",
          "iopub.status.idle": "2024-10-15T12:23:38.839922Z",
          "shell.execute_reply.started": "2024-10-15T12:23:38.832857Z",
          "shell.execute_reply": "2024-10-15T12:23:38.838623Z"
        },
        "trusted": true,
        "id": "qbjTubOMP9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_loader, model, loss_function, optimizer, epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for inputs, targets in data_loader:\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)  # Assuming inputs are in the right format\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_function(outputs, targets)\n",
        "            epoch_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(data_loader)}\")\n",
        "\n",
        "# Example usage\n",
        "epochs = 10\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()  # Example loss function\n",
        "\n",
        "train(dataloader, model, loss_function, optimizer, epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:54:34.033886Z",
          "iopub.execute_input": "2024-10-15T12:54:34.034399Z",
          "iopub.status.idle": "2024-10-15T12:54:34.194893Z",
          "shell.execute_reply.started": "2024-10-15T12:54:34.034355Z",
          "shell.execute_reply": "2024-10-15T12:54:34.193146Z"
        },
        "trusted": true,
        "id": "cXoxX7mkP9xn",
        "outputId": "b558e257-353b-4a3a-b00e-32b2c657ab8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[273], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# Example optimizer\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \u001b[38;5;66;03m# Example loss function\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[273], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, loss_function, optimizer, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming inputs are in the right format\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[259], line 12\u001b[0m, in \u001b[0;36mBiRNN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m (\u001b[38;5;28mself\u001b[39m, inputs) :\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m     14\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(outputs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ],
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error"
        }
      ]
    }
    {
      "cell_type": "code",
      "source": [
        "# add attention and dropout layers to the model\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T12:23:40.551356Z",
          "iopub.status.idle": "2024-10-15T12:23:40.551795Z",
          "shell.execute_reply.started": "2024-10-15T12:23:40.551555Z",
          "shell.execute_reply": "2024-10-15T12:23:40.551574Z"
        },
        "trusted": true,
        "id": "hP1cxU1rP9xn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
